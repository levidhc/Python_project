{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Convolution2D, Activation, MaxPooling2D, Dense, BatchNormalization, Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 30000\n",
    "training_inputs = X_train[0:N_train,:,:] / 255.0\n",
    "training_targets = np_utils.to_categorical(y_train)[0:N_train]\n",
    "\n",
    "val_inputs = X_train[(N_train+1):42000,:,:] / 255.0\n",
    "val_targets = np_utils.to_categorical(y_train)[(N_train+1):42000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = training_inputs.reshape(training_inputs.shape[0], 784)\n",
    "val_inputs = val_inputs.reshape(val_inputs.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer by layer pretraining Models (greedy layer-wise training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape = (784, ))\n",
    "distorted_input1 = Dropout(.1)(input_img)\n",
    "encoded1 = Dense(800, activation = 'sigmoid')(distorted_input1)\n",
    "encoded1_bn = BatchNormalization()(encoded1)\n",
    "decoded1 = Dense(784, activation = 'sigmoid')(encoded1_bn)\n",
    "\n",
    "autoencoder1 = Model(input=input_img, output=decoded1)\n",
    "encoder1 = Model(input=input_img, output=encoded1_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n"
     ]
    }
   ],
   "source": [
    "encoded1_input = Input(shape = (800,))\n",
    "distorted_input2 = Dropout(.2)(encoded1_input)\n",
    "encoded2 = Dense(400, activation='sigmoid')(distorted_input2)\n",
    "encoded2_bn = BatchNormalization()(encoded2)\n",
    "decoded2 = Dense(800, activation='sigmoid')(encoded2_bn)\n",
    "\n",
    "autoencoder2 = Model(input=encoded1_input, output=decoded2)\n",
    "encoder2 = Model(input=encoded1_input, output=encoded2_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n"
     ]
    }
   ],
   "source": [
    "encoded2_input = Input(shape = (400,))\n",
    "distorted_input3 = Dropout(.3)(encoded2_input)\n",
    "encoded3 = Dense(200, activation='sigmoid')(distorted_input3)\n",
    "encoded3_bn = BatchNormalization()(encoded3)\n",
    "decoded3 = Dense(400, activation='sigmoid')(encoded3_bn)\n",
    "\n",
    "autoencoder3 = Model(input=encoded2_input, output=decoded3)\n",
    "encoder3 = Model(input=encoded2_input, output=encoded3_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "encoded1_da = Dense(800, activation='sigmoid')(input_img)\n",
    "encoded1_da_bn = BatchNormalization()(encoded1_da)\n",
    "encoded2_da = Dense(400, activation='sigmoid')(encoded1_da_bn)\n",
    "encoded2_da_bn = BatchNormalization()(encoded2_da)\n",
    "encoded3_da = Dense(200, activation='sigmoid')(encoded2_da_bn)\n",
    "encoded3_da_bn = BatchNormalization()(encoded3_da)\n",
    "decoded3_da = Dense(400, activation='sigmoid')(encoded3_da_bn)\n",
    "decoded2_da = Dense(800, activation='sigmoid')(decoded3_da)\n",
    "decoded1_da = Dense(784, activation='sigmoid')(decoded2_da)\n",
    "\n",
    "deep_autoencoder = Model(input=input_img, output=decoded1_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "nad_encoded1_da = Dense(800, activation='sigmoid')(input_img)\n",
    "nad_encoded1_da_bn = BatchNormalization()(nad_encoded1_da)\n",
    "nad_encoded2_da = Dense(400, activation='sigmoid')(nad_encoded1_da_bn)\n",
    "nad_encoded2_da_bn = BatchNormalization()(nad_encoded2_da)\n",
    "nad_decoded2_da = Dense(800, activation='sigmoid')(nad_encoded2_da_bn)\n",
    "nad_decoded1_da = Dense(784, activation='sigmoid')(nad_decoded2_da)\n",
    "\n",
    "nad_deep_autoencoder = Model(input=input_img, output=nad_decoded1_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25\n",
      "3.6125\n",
      "3.0706249999999997\n",
      "2.6100312499999996\n",
      "2.2185265624999997\n",
      "1.8857475781249997\n",
      "1.60288544140625\n",
      "1.3624526251953124\n",
      "1.1580847314160156\n",
      "0.9843720217036133\n",
      "0.8367162184480713\n",
      "0.7112087856808607\n"
     ]
    }
   ],
   "source": [
    "sgd1 = SGD(lr = 5, decay = 0.5, momentum = 0.85, nesterov = True)\n",
    "sgd2 = SGD(lr = 5, decay = 0.5, momentum = 0.85, nesterov = True)\n",
    "sgd3 = SGD(lr = 5, decay = 0.5, momentum = 0.85, nesterov = True)\n",
    "\n",
    "autoencoder1.compile(loss='binary_crossentropy', optimizer = sgd1)\n",
    "autoencoder2.compile(loss='binary_crossentropy', optimizer = sgd2)\n",
    "autoencoder3.compile(loss='binary_crossentropy', optimizer = sgd3)\n",
    "\n",
    "deep_autoencoder.compile(loss='binary_crossentropy', optimizer = sgd1)\n",
    "nad_deep_autoencoder.compile(loss='binary_crossentropy', optimizer=sgd1)\n",
    "\n",
    "# what will happen to the learning rates under this decay schedule?\n",
    "lr = 5\n",
    "for i in range(12):\n",
    "    lr = lr - lr*.15\n",
    "    print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training first autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/8\n",
      "21000/21000 [==============================] - 4s 169us/step - loss: 0.5178 - val_loss: 0.3335\n",
      "Epoch 2/8\n",
      "21000/21000 [==============================] - 3s 148us/step - loss: 0.2945 - val_loss: 0.2196\n",
      "Epoch 3/8\n",
      "21000/21000 [==============================] - 3s 147us/step - loss: 0.2223 - val_loss: 0.1890\n",
      "Epoch 4/8\n",
      "21000/21000 [==============================] - 3s 149us/step - loss: 0.1944 - val_loss: 0.1756\n",
      "Epoch 5/8\n",
      "21000/21000 [==============================] - 3s 147us/step - loss: 0.1802 - val_loss: 0.1673\n",
      "Epoch 6/8\n",
      "21000/21000 [==============================] - 3s 149us/step - loss: 0.1714 - val_loss: 0.1616\n",
      "Epoch 7/8\n",
      "21000/21000 [==============================] - 3s 148us/step - loss: 0.1655 - val_loss: 0.1572\n",
      "Epoch 8/8\n",
      "21000/21000 [==============================] - 3s 148us/step - loss: 0.1611 - val_loss: 0.1540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2113dd6a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder1.fit(training_inputs, training_inputs, nb_epoch=8, batch_size=512, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 800)\n"
     ]
    }
   ],
   "source": [
    "first_layer_code = encoder1.predict(training_inputs)\n",
    "print(first_layer_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training second autoencoder\n",
    "encoder1에서 나온 first_layer_code를 second autoencoder의 input 및 output으로 넣어서 학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 7500 samples\n",
      "Epoch 1/8\n",
      "22500/22500 [==============================] - 2s 101us/step - loss: -1.4606 - val_loss: -3.1380\n",
      "Epoch 2/8\n",
      "22500/22500 [==============================] - 2s 93us/step - loss: -3.9531 - val_loss: -4.5780\n",
      "Epoch 3/8\n",
      "22500/22500 [==============================] - 2s 91us/step - loss: -4.7212 - val_loss: -4.8903\n",
      "Epoch 4/8\n",
      "22500/22500 [==============================] - 2s 90us/step - loss: -4.9186 - val_loss: -5.0081\n",
      "Epoch 5/8\n",
      "22500/22500 [==============================] - 2s 90us/step - loss: -5.0123 - val_loss: -5.0789\n",
      "Epoch 6/8\n",
      "22500/22500 [==============================] - 2s 91us/step - loss: -5.0713 - val_loss: -5.1299\n",
      "Epoch 7/8\n",
      "22500/22500 [==============================] - 2s 91us/step - loss: -5.1163 - val_loss: -5.1693\n",
      "Epoch 8/8\n",
      "22500/22500 [==============================] - 2s 93us/step - loss: -5.1509 - val_loss: -5.2015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2113dd5f8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder2.fit(first_layer_code, first_layer_code, nb_epoch=8, batch_size=512, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 400)\n"
     ]
    }
   ],
   "source": [
    "second_layer_code = encoder2.predict(first_layer_code)\n",
    "print(second_layer_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training third autoencoder\n",
    "encoder2에서 나온 second_layer_code를 third autoencoder의 input 및 output으로 넣어서 학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/8\n",
      "21000/21000 [==============================] - 1s 45us/step - loss: -11.3211 - val_loss: -15.7456\n",
      "Epoch 2/8\n",
      "21000/21000 [==============================] - 1s 38us/step - loss: -15.7859 - val_loss: -16.1216\n",
      "Epoch 3/8\n",
      "21000/21000 [==============================] - 1s 39us/step - loss: -16.0022 - val_loss: -16.2525\n",
      "Epoch 4/8\n",
      "21000/21000 [==============================] - 1s 37us/step - loss: -16.1033 - val_loss: -16.3328\n",
      "Epoch 5/8\n",
      "21000/21000 [==============================] - 1s 38us/step - loss: -16.1723 - val_loss: -16.3914\n",
      "Epoch 6/8\n",
      "21000/21000 [==============================] - 1s 35us/step - loss: -16.2243 - val_loss: -16.4373\n",
      "Epoch 7/8\n",
      "21000/21000 [==============================] - 1s 36us/step - loss: -16.2621 - val_loss: -16.4742\n",
      "Epoch 8/8\n",
      "21000/21000 [==============================] - 1s 34us/step - loss: -16.2951 - val_loss: -16.5043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2124fed30>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder3.fit(second_layer_code, second_layer_code, nb_epoch=8, batch_size=512, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the weights of the deep autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_autoencoder.layers[1].set_weights(autoencoder1.layers[2].get_weights()) # first dense layer (800)\n",
    "deep_autoencoder.layers[2].set_weights(autoencoder1.layers[3].get_weights()) # first bn layer\n",
    "deep_autoencoder.layers[3].set_weights(autoencoder2.layers[2].get_weights()) # second dense layer\n",
    "deep_autoencoder.layers[4].set_weights(autoencoder2.layers[3].get_weights()) # second bn layer\n",
    "deep_autoencoder.layers[5].set_weights(autoencoder3.layers[2].get_weights()) # third dense layer\n",
    "deep_autoencoder.layers[6].set_weights(autoencoder3.layers[3].get_weights()) # third bn layer\n",
    "deep_autoencoder.layers[7].set_weights(autoencoder3.layers[4].get_weights()) # first decoder\n",
    "deep_autoencoder.layers[8].set_weights(autoencoder2.layers[4].get_weights()) # second decoder\n",
    "deep_autoencoder.layers[9].set_weights(autoencoder1.layers[4].get_weights()) # third decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the weights of the not-as-deep autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nad_deep_autoencoder.layers[1].set_weights(autoencoder1.layers[2].get_weights()) # first dense layer\n",
    "nad_deep_autoencoder.layers[2].set_weights(autoencoder1.layers[3].get_weights()) # first bn layer\n",
    "nad_deep_autoencoder.layers[3].set_weights(autoencoder2.layers[2].get_weights()) # second dense layer\n",
    "nad_deep_autoencoder.layers[4].set_weights(autoencoder2.layers[3].get_weights()) # second bn layer\n",
    "nad_deep_autoencoder.layers[5].set_weights(autoencoder2.layers[4].get_weights()) # second decoder\n",
    "nad_deep_autoencoder.layers[6].set_weights(autoencoder1.layers[4].get_weights()) # third decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_inputs = nad_deep_autoencoder.predict(training_inputs[0:25,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15118212,  0.18637228,  0.1645885 , ...,  0.13445844,\n",
       "         0.22603853,  0.29167011],\n",
       "       [ 0.21193783,  0.2805427 ,  0.17357469, ...,  0.18749757,\n",
       "         0.22023115,  0.38302997],\n",
       "       [ 0.16627672,  0.21487252,  0.253699  , ...,  0.13204175,\n",
       "         0.18625297,  0.16529225],\n",
       "       ..., \n",
       "       [ 0.20453706,  0.15460625,  0.20954278, ...,  0.13629699,\n",
       "         0.13651608,  0.22064735],\n",
       "       [ 0.27779889,  0.19214535,  0.2262421 , ...,  0.1863195 ,\n",
       "         0.19440638,  0.4094469 ],\n",
       "       [ 0.22172701,  0.25243217,  0.15116775, ...,  0.11172511,\n",
       "         0.19175291,  0.22743715]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to 'fine-tuning' for classification \n",
    "전체 모델을 supervised learning으로 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 7500 samples\n",
      "Epoch 1/6\n",
      "22500/22500 [==============================] - 5s 217us/step - loss: 1.1087 - acc: 0.6598 - val_loss: 0.4034 - val_acc: 0.8712\n",
      "Epoch 2/6\n",
      "22500/22500 [==============================] - 4s 188us/step - loss: 0.2995 - acc: 0.9072 - val_loss: 0.2508 - val_acc: 0.9213\n",
      "Epoch 3/6\n",
      "22500/22500 [==============================] - 4s 195us/step - loss: 0.1922 - acc: 0.9412 - val_loss: 0.1989 - val_acc: 0.9372\n",
      "Epoch 4/6\n",
      "22500/22500 [==============================] - 4s 196us/step - loss: 0.1378 - acc: 0.9576 - val_loss: 0.1837 - val_acc: 0.9433\n",
      "Epoch 5/6\n",
      "22500/22500 [==============================] - 4s 191us/step - loss: 0.1097 - acc: 0.9668 - val_loss: 0.1557 - val_acc: 0.9531\n",
      "Epoch 6/6\n",
      "22500/22500 [==============================] - 4s 188us/step - loss: 0.0819 - acc: 0.9752 - val_loss: 0.1577 - val_acc: 0.9501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f212b396a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1 = Dense(500, activation='relu')(nad_decoded1_da)\n",
    "dense1_drop = Dropout(.3)(dense1)\n",
    "dense2 = Dense(10, activation='sigmoid')(dense1_drop)\n",
    "\n",
    "classifier = Model(input=input_img, output=dense2)\n",
    "sgd4 = SGD(lr=.1, decay=0.001, momentum=0.95, nesterov=True)\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer=sgd4, metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(training_inputs, training_targets, nb_epoch=6, batch_size=600, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = classifier.predict(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 9, 6, 4, 4, 1, 7, 0, 9, 3, 5, 8, 2, 7, 9, 7, 7, 8, 5, 7, 9, 6,\n",
       "       6, 6], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(val_preds, axis=1)\n",
    "true_digits = np.argmax(val_targets, axis=1)\n",
    "predictions[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 9, 6, 4, 4, 1, 7, 0, 9, 3, 5, 8, 2, 7, 4, 7, 7, 8, 5, 7, 9, 6,\n",
       "       6, 6], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_digits[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
